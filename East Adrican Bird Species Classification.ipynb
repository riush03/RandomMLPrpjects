{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bfdc97",
   "metadata": {},
   "source": [
    "<b>Table of content</b>\n",
    "<ul>\n",
    "    <li>Overview</li>\n",
    "    <li>Importing the required libraries</li>\n",
    "    <li>Configuration</li>\n",
    "    <li>Helper functions</li>\n",
    "    <li>Loading the data</li>\n",
    "    <li>Create Tensorflow Dataset</li>\n",
    "    <li>Model Development</li>\n",
    "    <li>Model Evaluation</li>\n",
    "    <li>Submit</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_io as tfio\n",
    "import IPython.display import Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import Ipython.display as ipd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ed8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration \n",
    "class CFG:\n",
    "    image_size = [256,256]\n",
    "    is_training = False\n",
    "    epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load audio files\n",
    "def load_audio(f_apth):\n",
    "    input_len = 32000\n",
    "    data = librosa.core.load(f_path)[0]\n",
    "    if len(data)>input_len:\n",
    "        data = data[:input_len]\n",
    "    else:\n",
    "        data = np.pad(data,(0,max(0,input_len - len(data))),\"constant\")\n",
    "        \n",
    "    return data\n",
    "\n",
    "def visualize_data(data):\n",
    "    fig = plt.figure(figsize=14,8)\n",
    "    plt.title('Raw Wave')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.plot(np.linspace(0,1,len(data)),data)\n",
    "    plt.show()\n",
    "    #sample usage\n",
    "    #data = load_audio_file(\"dir\")\n",
    "    #visualize_data(data)\n",
    "    #ipd.Audio(data,rate=32000)\n",
    "    #other things to do -> add whitenoise stretch sound\n",
    "    \n",
    "#precosessing auido data\n",
    "def preprocess(audio_dir,label):\n",
    "    audio_string = tf.io.read_file(audio_dir)\n",
    "    audio = tfio.audio.decode_vorbis(audio_string)\n",
    "    audio_tensor = tf.squeeze(audio,axis=[-1])\n",
    "    diff = tf.cast(tf.shape(audio_tensor)[0] - 5 * 32000,tf.float32)\n",
    "    start  = tf.cast(tf.random.uniform(shape=()) * diff, tf.int32)\n",
    "    current = tf.where(diff>0,begin,0)\n",
    "    stop = tf.where(diff > 0,current + 5 * 32000,tf.shape(audio_tensor)[0])\n",
    "    audio_tensor = audio_tensor[start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecComputer:\n",
    "    def __init__(self,sr,n_mels,f_min,f_max,**kwargs):\n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        kwargs[\"n_fft\"] = kwargs.get(\"n_fft\",self.sr//(10*4))\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def __call__(self,y):\n",
    "        mel_spec = lb.feature.melspectrogram(y,\n",
    "                                            sr=self.sr,\n",
    "                                            n_mels=self.n_mels,\n",
    "                                            f_min=self.f_min,\n",
    "                                            f_max=self.f_max,\n",
    "                                            **self.kwargs)\n",
    "        mel_spec = lb.power_to_db(mel_spec).astype(np.float32)\n",
    "        return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random integer\n",
    "def random_int(shape=[],min_val=0,max_val=1):\n",
    "    return tf.random.uniform(shape=shape,min_val=min_val,max_val=max_val,dtype=tf.int32)\n",
    "\n",
    "def mono_to_color(X,eps=1e-6,mean=None,std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    _min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def crop_pad(audio,tag_len,pad_mode='constant'):\n",
    "    #check the length of the imported audio\n",
    "    audio_len = tf.shape(audio)[0]\n",
    "    #if the length  of the input audio is smaller than the target length ,randomly pad the audio\n",
    "    if audio_len < tag_len:\n",
    "        #calculate the offset between he input audio and the target length\n",
    "        diff_len = (tag_len - audio_len)\n",
    "        pad1 = random_int([] ,minval=0,maxval=diff_len)\n",
    "        \n",
    "        #calcualte the second padding value\n",
    "        pad2 = diff_len - pad1\n",
    "        pad_len = [pad1,pad2]\n",
    "        \n",
    "        \n",
    "        #apply padding to the audio data\n",
    "        audio = tf.pad(audio,paddings=[pad_len],mode=pad_mode)\n",
    "        \n",
    "        elif audio > tag_len:\n",
    "            diff_len = (audio_len - tag_len)\n",
    "            indx = tf.random.uniform([],0,diff_len,dtype=tf.int32)\n",
    "            #crop the audio data\n",
    "            audio = audio[indx: (indx + tag_len)]\n",
    "            #reshape the audio data to the target length\n",
    "            audio = tf.reshape(audio,[target_len])\n",
    "            \n",
    "            return audio\n",
    "        \n",
    "@tf.function\n",
    "def normalize(data,min_max=True):\n",
    "    #compute the mean and std of the data\n",
    "    me_an = tf.math.reduce_mean(data)\n",
    "    std = tf.math.reduce_std(data)\n",
    "    #standardize the data\n",
    "    data = tf.math.divide_no_nan(data -mean,std)\n",
    "    #nomarlize b2n [0,1]\n",
    "    if min_max:\n",
    "        MIN = tf.math.reduce_min(data)\n",
    "        MAX = tf.math.reduce_max(data)\n",
    "        data = tf.math.divide_no_nan(data - MIN, MAX-MIN)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524132d2",
   "metadata": {},
   "source": [
    "# EA Birds Tensorflow Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EABirdsDsLoader:\n",
    "    def __init(self,df,train_path,sr=32000,duration=10,audio_len = 10 *32000,window_size=1024,n_mels=128,n_fft=2048,hop_size=512,\n",
    "              batch_size=32,shuffle=True):\n",
    "        self.train_path = train_path\n",
    "        self.df_path = df_path\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.audio_len = audio_len\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_size = hop_size\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.shuffle = shuffle\n",
    "        self.df = pd.read_csv(self.df_path)\n",
    "        \n",
    "        #create a list of images paths and labels for train,validation and test dataset\n",
    "        self.audio_paths = [os.path.join(self.train_path,fname) for fname in self.df.filename]\n",
    "        self.labels = self.df.primary_label.values\n",
    "        self.indx_labels = {}\n",
    "        \n",
    "    def load_file(self,f_path,label):\n",
    "        audio = tf.io.read_file(f_path)\n",
    "        audio = tfio.audio.decode_vorbis(audio)\n",
    "        audi = tf.cast(audio,tf.float32)\n",
    "        audio = tf.squeeze(audio,axis=-1)\n",
    "        audio = crop_paf(audio,self.audio_len)\n",
    "        if normalize:\n",
    "            audio = normalize(audio)\n",
    "        return audio,label\n",
    "    \n",
    "    def spectrog_2_img(self,spectogram,label):\n",
    "        spectrogram = tf.expand_dims(spectogram,axis=-1)\n",
    "        spectogram = tf.image.resize(spectogram,[224,224])\n",
    "        spectro_gram = tf.image.grayyscale_to_rgb(spectogram)\n",
    "        return spectogram,label\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((self.audio_paths,self.labels))\n",
    "        ds = ds.map(self.load_audio,num_parrallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.map(AudioAug,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.map(self.spectrog_2_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            ds = ds.shuffle(buff_size=500,reshuffle_each_iteration=True)\n",
    "            ds = ds.batch(batch_size.self.batch_size)\n",
    "            ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = EABirdsDsLoader(df_path='',train_path)\n",
    "train_ds = train_data_loader.get_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
